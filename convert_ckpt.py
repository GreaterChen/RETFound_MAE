#!/usr/bin/env python3
"""
RETFound PyTorchæƒé‡åŠ è½½åˆ°PaddlePaddleç¤ºä¾‹è„šæœ¬

æ­¤è„šæœ¬æ¼”ç¤ºäº†å¦‚ä½•å°†RETFoundçš„PyTorché¢„è®­ç»ƒæƒé‡è½¬æ¢å¹¶åŠ è½½åˆ°PaddlePaddleæ¨¡å‹ä¸­ã€‚
æ”¯æŒä»HuggingFace Hubæˆ–æœ¬åœ°æ–‡ä»¶åŠ è½½æƒé‡ï¼Œæ”¯æŒé•œåƒç«™å’Œtokenè®¤è¯ã€‚
"""

import os
import time
import argparse
import paddle
import numpy as np
from models_vit import RETFound_mae, RETFound_dinov2
from util.weight_converter import convert_retfound_weights, load_converted_weights_to_model


def setup_huggingface_config(args):
    """
    è®¾ç½®HuggingFaceé…ç½®ï¼ŒåŒ…æ‹¬é•œåƒç«™å’Œtokenç™»å½•
    
    Args:
        args: å‘½ä»¤è¡Œå‚æ•°å¯¹è±¡
    """
    try:
        from huggingface_hub import login
        
        # è®¾ç½®HFé•œåƒç«™
        hf_endpoint = args.hf_endpoint or os.getenv('HF_ENDPOINT')
        if hf_endpoint:
            print(f"ä½¿ç”¨ HF é•œåƒç«™: {hf_endpoint}")
            os.environ['HF_ENDPOINT'] = hf_endpoint
        
        # ç™»å½• Hugging Face
        hf_token = args.hf_token or os.getenv('HF_TOKEN')
        if hf_token:
            print("æ­£åœ¨ç™»å½• Hugging Face...")
            login(token=hf_token)
            print("âœ… Hugging Face ç™»å½•æˆåŠŸ")
        else:
            print("âš ï¸  è­¦å‘Š: æœªæä¾› Hugging Face tokenï¼Œå¯èƒ½æ— æ³•è®¿é—®å—é™ä»“åº“")
            print("ä½ å¯ä»¥ä½¿ç”¨ --hf_token å‚æ•°æˆ–è®¾ç½® HF_TOKEN ç¯å¢ƒå˜é‡")
        
        return hf_endpoint
        
    except ImportError:
        print("è¯·å®‰è£…huggingface_hub: pip install huggingface_hub")
        return None


def download_huggingface_model(model_name, cache_dir="./models", hf_endpoint=None, max_retries=3):
    """
    ä»HuggingFace Hubä¸‹è½½RETFoundæ¨¡å‹æƒé‡ï¼Œæ”¯æŒé•œåƒç«™å’Œé‡è¯•æœºåˆ¶
    
    Args:
        model_name: æ¨¡å‹åç§°ï¼Œå¦‚ "ycxia/RETFound_MAE"
        cache_dir: æ¨¡å‹ç¼“å­˜ç›®å½•
        hf_endpoint: HuggingFaceé•œåƒç«™åœ°å€
        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
    """
    try:
        from huggingface_hub import hf_hub_download
        import torch
        
        print(f"æ­£åœ¨ä» HuggingFace Hub ä¸‹è½½ {model_name}...")
        
        # ä¸‹è½½æ¨¡å‹æƒé‡æ–‡ä»¶ï¼Œæ”¯æŒé‡è¯•æœºåˆ¶
        for attempt in range(max_retries):
            try:
                download_kwargs = {
                    'repo_id': model_name,
                    'filename': "pytorch_model.bin",  # æˆ–å°è¯•å…¶ä»–å¯èƒ½çš„æ–‡ä»¶å
                    'cache_dir': cache_dir
                }
                
                # å¦‚æœæŒ‡å®šäº†é•œåƒç«™åœ°å€ï¼Œæ·»åŠ åˆ°å‚æ•°ä¸­
                if hf_endpoint:
                    download_kwargs['endpoint'] = hf_endpoint
                
                model_file = hf_hub_download(**download_kwargs)
                print(f"âœ… æ¨¡å‹ä¸‹è½½æˆåŠŸ: {model_file}")
                return model_file
                
            except Exception as e:
                print(f"ä¸‹è½½å°è¯• {attempt + 1}/{max_retries} å¤±è´¥: {str(e)}")
                
                # å¦‚æœæ˜¯æ–‡ä»¶åé—®é¢˜ï¼Œå°è¯•å…¶ä»–å¯èƒ½çš„æ–‡ä»¶å
                if "does not exist" in str(e) and "pytorch_model.bin" in str(e):
                    try:
                        download_kwargs['filename'] = "model.safetensors"
                        model_file = hf_hub_download(**download_kwargs)
                        print(f"âœ… ä½¿ç”¨ safetensors æ ¼å¼ä¸‹è½½æˆåŠŸ: {model_file}")
                        return model_file
                    except:
                        pass
                
                if attempt == max_retries - 1:
                    print("æ‰€æœ‰ä¸‹è½½å°è¯•éƒ½å¤±è´¥äº†ï¼Œè¯·å°è¯•:")
                    print("1. ä½¿ç”¨ --hf_endpoint https://hf-mirror.com")
                    print("2. è®¾ç½®ç¯å¢ƒå˜é‡: export HF_ENDPOINT=https://hf-mirror.com")
                    print("3. æ£€æŸ¥ç½‘ç»œè¿æ¥")
                    print("4. ç¡®è®¤æ¨¡å‹åç§°æ˜¯å¦æ­£ç¡®")
                    raise e
                else:
                    print(f"5ç§’åé‡è¯•...")
                    time.sleep(5)
        
        return None
        
    except ImportError:
        print("è¯·å®‰è£…huggingface_hub: pip install huggingface_hub")
        return None
    except Exception as e:
        print(f"ä¸‹è½½æ¨¡å‹å¤±è´¥: {e}")
        return None


def load_retfound_mae_example(args):
    """RETFound MAEæ¨¡å‹åŠ è½½ç¤ºä¾‹"""
    print("=== RETFound MAE æ¨¡å‹åŠ è½½ç¤ºä¾‹ ===")
    
    # 1. åˆ›å»ºPaddlePaddleæ¨¡å‹
    model = RETFound_mae(num_classes=5)
    print(f"åˆ›å»º RETFound MAE æ¨¡å‹ï¼Œå‚æ•°é‡: {sum(p.numel() for p in model.parameters())}")
    
    # 2. é€‰æ‹©æƒé‡æ¥æº
    pytorch_model_path = None
    
    # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨æœ¬åœ°æ¨¡å¼
    if not args.use_huggingface:  # use_localæ¨¡å¼
        print("ğŸ“ ä½¿ç”¨æœ¬åœ°pthæ–‡ä»¶æ¨¡å¼")
        
        if not args.local_model_path:
            print("âŒ é”™è¯¯: æœ¬åœ°æ¨¡å¼éœ€è¦æŒ‡å®š --local_model_path å‚æ•°")
            print("ä½¿ç”¨æ–¹å¼: python script.py --use_local --local_model_path /path/to/model.pth")
            return None
        
        pytorch_model_path = args.local_model_path
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(pytorch_model_path):
            print(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {pytorch_model_path}")
            return None
        
        if not os.path.isfile(pytorch_model_path):
            print(f"âŒ æŒ‡å®šè·¯å¾„ä¸æ˜¯æ–‡ä»¶: {pytorch_model_path}")
            return None
        
        print(f"âœ… æ‰¾åˆ°æ¨¡å‹æ–‡ä»¶: {pytorch_model_path}")
            
    else:  # HuggingFaceä¸‹è½½æ¨¡å¼
        print("ğŸŒ ä½¿ç”¨ HuggingFace ä¸‹è½½æ¨¡å¼")
        # è®¾ç½®HuggingFaceé…ç½®ï¼ˆåŒ…æ‹¬ç™»å½•ç­‰ï¼‰
        hf_endpoint = setup_huggingface_config(args)
        pytorch_model_path = download_huggingface_model(
            args.model_name, 
            cache_dir=args.cache_dir,
            hf_endpoint=hf_endpoint,
            max_retries=args.max_retries
        )
    
    if pytorch_model_path is None:
        print("æ— æ³•è·å–PyTorchæ¨¡å‹æƒé‡")
        return None
    
    # 3. è½¬æ¢å¹¶åŠ è½½æƒé‡
    try:
        print(f"ğŸ”„ å¼€å§‹åŠ è½½æƒé‡: {pytorch_model_path}")
        success = model.load_pytorch_weights(pytorch_model_path)
        
        if success:
            print("âœ… æƒé‡åŠ è½½æˆåŠŸï¼")
            
            # 4. æµ‹è¯•æ¨¡å‹æ¨ç†
            if args.test_inference:
                test_model_inference(model)
            
            return model
        else:
            print("âŒ æƒé‡åŠ è½½å¤±è´¥")
            return None
            
    except Exception as e:
        print(f"åŠ è½½æƒé‡æ—¶å‡ºé”™: {e}")
        return None


def test_model_inference(model):
    """æµ‹è¯•æ¨¡å‹æ¨ç†"""
    print("\n=== æµ‹è¯•æ¨¡å‹æ¨ç† ===")
    
    # åˆ›å»ºéšæœºè¾“å…¥
    batch_size = 2
    input_tensor = paddle.randn([batch_size, 3, 224, 224])
    
    # è®¾ç½®è¯„ä¼°æ¨¡å¼
    model.eval()
    
    with paddle.no_grad():
        # å‰å‘ä¼ æ’­
        output = model(input_tensor)
        print(f"è¾“å…¥å½¢çŠ¶: {input_tensor.shape}")
        print(f"è¾“å‡ºå½¢çŠ¶: {output.shape}")
        print(f"è¾“å‡ºèŒƒå›´: [{output.min().item():.4f}, {output.max().item():.4f}]")
        
        # è·å–ç‰¹å¾
        features = model.forward_features(input_tensor)
        print(f"ç‰¹å¾å½¢çŠ¶: {features.shape}")
        
    print("âœ… æ¨¡å‹æ¨ç†æµ‹è¯•é€šè¿‡ï¼")


def parse_arguments():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description='RETFound PyTorchæƒé‡è½¬æ¢å’ŒåŠ è½½å·¥å…·')
    
    # HuggingFace ç›¸å…³å‚æ•°
    parser.add_argument('--hf_token', default=None, type=str, 
                        help='Hugging Face token for accessing gated repositories')
    parser.add_argument('--hf_endpoint', default="https://hf-mirror.com", type=str,
                        help='Hugging Face endpoint URL (use https://hf-mirror.com for China mirror)')
    
    # æ¨¡å‹ç›¸å…³å‚æ•°
    parser.add_argument('--model_name', default="YukunZhou/RETFound_mae_meh", type=str,
                        help='HuggingFace model repository name')
    parser.add_argument('--cache_dir', default="./models", type=str,
                        help='Directory to cache downloaded models')
    parser.add_argument('--local_model_path', type=str, default="/home/chenlb24/RETFound_MAE/RETFound_mae_meh.pth",
                        help='Path to local PyTorch model file')
    
    # ä¸‹è½½å’Œå¤„ç†å‚æ•°
    parser.add_argument('--max_retries', default=3, type=int,
                        help='Maximum number of download retries')
    parser.add_argument('--use_huggingface', action='store_true', default=False,
                        help='Download from HuggingFace Hub (default: True)')
    parser.add_argument('--use_local', dest='use_huggingface', action='store_false', default=True,
                        help='Use local model file instead of downloading')
    
    # åŠŸèƒ½é€‰é¡¹
    parser.add_argument('--test_inference', action='store_true', default=True,
                        help='Test model inference after loading')
    parser.add_argument('--validation_samples', default=5, type=int,
                        help='Number of samples for validation testing')
    parser.add_argument('--save_paddle_model', action='store_true', default=False,
                        help='Save converted model in PaddlePaddle format (default: False, since model.load_pytorch_weights already saves converted weights)')
    parser.add_argument('--paddle_model_path', default=None, type=str,
                        help='Path to save PaddlePaddle model (default: auto-generated based on input filename)')
    
    # è®¾å¤‡å‚æ•°
    parser.add_argument('--device', default='auto', choices=['auto', 'gpu', 'cpu'],
                        help='Device to use for computation')
    
    return parser.parse_args()


def main():
    """ä¸»å‡½æ•°"""
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    args = parse_arguments()
    
    print("RETFound PyTorchæƒé‡è½¬æ¢å’ŒåŠ è½½ç¤ºä¾‹")
    print("=" * 50)
    print("ğŸ“ æ³¨æ„: æƒé‡è½¬æ¢è¿‡ç¨‹è¯´æ˜")
    print("   1. model.load_pytorch_weights() ä¼šè‡ªåŠ¨è½¬æ¢å¹¶ä¿å­˜æƒé‡æ–‡ä»¶")
    print("   2. å¦‚æœå¯ç”¨ --save_paddle_modelï¼Œè„šæœ¬ä¼šæ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨æ–‡ä»¶ï¼Œé¿å…é‡å¤ä¿å­˜")
    print("   3. é»˜è®¤æƒ…å†µä¸‹åªä¿å­˜ä¸€ä»½è½¬æ¢åçš„æƒé‡æ–‡ä»¶")
    print("=" * 50)
    print()
    
    # æ˜¾ç¤ºå½“å‰æ¨¡å¼ä¿¡æ¯
    if not args.use_huggingface:  # æœ¬åœ°æ¨¡å¼
        print("ğŸ  æ¨¡å¼: æœ¬åœ°æ–‡ä»¶æ¨¡å¼")
        if args.local_model_path:
            print(f"ğŸ“ æœ¬åœ°æ¨¡å‹è·¯å¾„: {args.local_model_path}")
        else:
            print("âš ï¸  æœªæŒ‡å®šæœ¬åœ°æ¨¡å‹è·¯å¾„")
    else:  # HuggingFaceæ¨¡å¼
        print("ğŸŒ æ¨¡å¼: HuggingFace ä¸‹è½½æ¨¡å¼")
        print(f"ğŸ“¦ æ¨¡å‹åç§°: {args.model_name}")
        print(f"ğŸ“‚ ç¼“å­˜ç›®å½•: {args.cache_dir}")
        if args.hf_endpoint:
            print(f"ğŸŒ HFé•œåƒç«™: {args.hf_endpoint}")
    
    # è®¾ç½®PaddlePaddleè®¾å¤‡
    if args.device == 'auto':
        device = 'gpu' if paddle.is_compiled_with_cuda() else 'cpu'
    else:
        device = args.device
    
    paddle.set_device(device)
    print(f"ğŸ–¥ï¸  ä½¿ç”¨è®¾å¤‡: {paddle.get_device()}")
    print()
    
    # ç¤ºä¾‹1: åŠ è½½RETFound MAEæ¨¡å‹
    model = load_retfound_mae_example(args)
    
    if model is not None:
        print("\nğŸ‰ RETFoundæ¨¡å‹å·²æˆåŠŸåŠ è½½åˆ°PaddlePaddle!")
        print("\nä½ ç°åœ¨å¯ä»¥ä½¿ç”¨è¿™ä¸ªæ¨¡å‹è¿›è¡Œ:")
        print("- å¾®è°ƒè®­ç»ƒ")
        print("- ç‰¹å¾æå–") 
        print("- çœ¼åº•å›¾åƒåˆ†æ")
        print("- è¿ç§»å­¦ä¹ ")
        
        # ä¿å­˜PaddlePaddleæ ¼å¼çš„æ¨¡å‹
        if args.save_paddle_model:
            # å¦‚æœæ²¡æœ‰æŒ‡å®šä¿å­˜è·¯å¾„ï¼Œåˆ™æ ¹æ®è¾“å…¥æ–‡ä»¶è‡ªåŠ¨ç”Ÿæˆ
            if args.paddle_model_path is None:
                if not args.use_huggingface and args.local_model_path:
                    base_name = os.path.splitext(os.path.basename(args.local_model_path))[0]
                    args.paddle_model_path = f"{base_name}_paddle.pdparams"
                else:
                    args.paddle_model_path = "retfound_mae_paddle.pdparams"
            
            paddle.save(model.state_dict(), args.paddle_model_path)
            print(f"ğŸ’¾ PaddlePaddleæ¨¡å‹å·²ä¿å­˜è‡³: {args.paddle_model_path}")
        
    else:
        if not args.use_huggingface:
            print("\nâŒ æœ¬åœ°æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥:")
            print("1. æ–‡ä»¶è·¯å¾„æ˜¯å¦æ­£ç¡®")
            print("2. æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨")
            print("3. æ–‡ä»¶æ ¼å¼æ˜¯å¦æ­£ç¡®")
        else:
            print("\nâŒ HuggingFaceæ¨¡å‹ä¸‹è½½/åŠ è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥:")
            print("1. ç½‘ç»œè¿æ¥")
            print("2. æ¨¡å‹åç§°æ˜¯å¦æ­£ç¡®")
            print("3. æƒé‡æ–‡ä»¶å’Œè½¬æ¢è¿‡ç¨‹")
        return 1
    
    return 0


if __name__ == "__main__":
    exit_code = main()
    exit(exit_code) 